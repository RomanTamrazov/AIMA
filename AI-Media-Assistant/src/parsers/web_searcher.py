#!/usr/bin/env python3
"""
–†–µ–∞–ª—å–Ω—ã–π –≤–µ–±-–ø–æ–∏—Å–∫ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π —á–µ—Ä–µ–∑ –ø–∞—Ä—Å–∏–Ω–≥ –∂–∏–≤—ã—Ö —Å–∞–π—Ç–æ–≤
"""

import aiohttp
import asyncio
import json
import re
from datetime import datetime, timedelta
from urllib.parse import quote, urljoin
import time
import random
from bs4 import BeautifulSoup
import logging
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RealWebSearcher:
    """–†–µ–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π —á–µ—Ä–µ–∑ –ø–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–æ–≤"""
    
    def __init__(self):
        self.session = None
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        ]
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä–µ–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        self.event_sources = [
            # –ê–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π
            {
                "name": "TimePad Technology",
                "url": "https://timepad.ru/events/categories/technology/",
                "type": "aggregator"
            },
            {
                "name": "TimePad Business", 
                "url": "https://timepad.ru/events/categories/business/",
                "type": "aggregator"
            },
            {
                "name": "TimePad Education",
                "url": "https://timepad.ru/events/categories/education/", 
                "type": "aggregator"
            },
            {
                "name": "Eventbrite SPb Tech",
                "url": "https://www.eventbrite.com/d/russia--saint-petersburg/technology--events/",
                "type": "aggregator"
            },
            {
                "name": "KudaGo –°–ü–± –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è",
                "url": "https://kudago.com/spb/list/meropriyatiya/",
                "type": "aggregator"
            },
            {
                "name": "–ê—Ñ–∏—à–∞ –°–ü–±",
                "url": "https://www.afisha.ru/spb/events/",
                "type": "aggregator"
            },
            
            # –£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—ã
            {
                "name": "–ò–¢–ú–û —Å–æ–±—ã—Ç–∏—è",
                "url": "https://events.itmo.ru/events",
                "type": "university"
            },
            {
                "name": "–°–ü–±–ì–£ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è", 
                "url": "https://events.spbu.ru/",
                "type": "university"
            },
            {
                "name": "–°–ü–±–ü–£ —Å–æ–±—ã—Ç–∏—è",
                "url": "https://www.spbstu.ru/events/",
                "type": "university"
            },
            {
                "name": "–õ–≠–¢–ò –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è",
                "url": "https://etu.ru/ru/universitet/meropriyatiya",
                "type": "university" 
            },
            {
                "name": "–ì–£–ê–ü —Å–æ–±—ã—Ç–∏—è",
                "url": "https://guap.ru/events",
                "type": "university"
            },
            
            # IT –∫–æ–º–ø–∞–Ω–∏–∏
            {
                "name": "–Ø–Ω–¥–µ–∫—Å —Å–æ–±—ã—Ç–∏—è",
                "url": "https://events.yandex.ru/",
                "type": "company"
            },
            {
                "name": "JetBrains —Å–æ–±—ã—Ç–∏—è",
                "url": "https://www.jetbrains.com/ru-ru/events/", 
                "type": "company"
            },
            {
                "name": "–°–±–µ—Ä –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è",
                "url": "https://sber.ru/events",
                "type": "company"
            },
            {
                "name": "–¢–∏–Ω—å–∫–æ—Ñ—Ñ —Å–æ–±—ã—Ç–∏—è",
                "url": "https://www.tinkoff.ru/events/",
                "type": "company"
            },
            {
                "name": "VK –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è",
                "url": "https://vk.com/events",
                "type": "company"
            },
            {
                "name": "Kaspersky —Å–æ–±—ã—Ç–∏—è",
                "url": "https://www.kaspersky.ru/events",
                "type": "company"
            },
            
            # IT –ø–æ—Ä—Ç–∞–ª—ã –∏ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞
            {
                "name": "–•–∞–±—Ä —Å–æ–±—ã—Ç–∏—è",
                "url": "https://habr.com/ru/hub/events/",
                "type": "community"
            },
            {
                "name": "VC.ru —Å–æ–±—ã—Ç–∏—è",
                "url": "https://vc.ru/events", 
                "type": "community"
            },
            {
                "name": "TAdviser –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è",
                "url": "https://www.tadviser.ru/index.php/–ú–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è",
                "type": "community"
            },
            {
                "name": "CNews —Å–æ–±—ã—Ç–∏—è",
                "url": "https://www.cnews.ru/events/",
                "type": "community"
            },
            {
                "name": "ODS.ai —Å–æ–±—ã—Ç–∏—è",
                "url": "https://ods.ai/events",
                "type": "community"
            },
            {
                "name": "DataFest —Å–æ–±—ã—Ç–∏—è",
                "url": "https://datafest.ru/events/",
                "type": "community"
            },
            
            # –ö–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏
            {
                "name": "CodeFest",
                "url": "https://codefest.ru/",
                "type": "conference"
            },
            {
                "name": "Heilum",
                "url": "https://heilum.ru/",
                "type": "conference"
            },
            {
                "name": "RootConf",
                "url": "https://rootconf.ru/",
                "type": "conference"
            },
            {
                "name": "FrontendConf",
                "url": "https://frontendconf.ru/",
                "type": "conference"
            },
            
            # –°—Ç–∞—Ä—Ç–∞–ø —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞
            {
                "name": "StartupSPB",
                "url": "https://startupspb.com/events/",
                "type": "startup"
            },
            {
                "name": "PiterStartup",
                "url": "https://piterstartup.ru/events/", 
                "type": "startup"
            },
            {
                "name": "Skolkovo —Å–æ–±—ã—Ç–∏—è",
                "url": "https://sk.ru/events/",
                "type": "startup"
            },
            
            # –ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ
            {
                "name": "IT Dialog",
                "url": "https://it-dialog.ru/",
                "type": "government"
            },
            {
                "name": "Digital SPb",
                "url": "https://digital.spb.ru/events/",
                "type": "government"
            }
        ]
        
        self.found_events = set()
    
    async def search_real_events(self, query="", max_events=100, days_ahead=90):
        """
        –†–µ–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π —á–µ—Ä–µ–∑ –ø–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–æ–≤
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            max_events: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π  
            days_ahead: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –≤–ø–µ—Ä–µ–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞
            
        Returns:
            List[dict]: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π
        """
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π: '{query}'")
        
        if not self.session:
            self.session = aiohttp.ClientSession()
        
        try:
            all_events = []
            
            # 1. –ü–∞—Ä—Å–∏–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            logger.info(f"üîç –ü–∞—Ä—Å–∏–º {len(self.event_sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
            parsed_events = await self._parse_all_sources(query)
            all_events.extend(parsed_events)
            
            # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            logger.info("üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –∏–∑ –±–∞–∑—ã...")
            db_events = self._load_events_from_database()
            all_events.extend(db_events)
            
            # 3. –§–∏–ª—å—Ç—Ä—É–µ–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
            filtered_events = self._filter_and_sort_events(all_events, max_events, days_ahead)
            
            logger.info(f"‚úÖ –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ {len(filtered_events)} —Ä–µ–∞–ª—å–Ω—ã—Ö –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π")
            return filtered_events
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return self._load_events_from_database()[:max_events]
    
    async def _parse_all_sources(self, query):
        """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π"""
        all_events = []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –≥—Ä—É–ø–ø—ã –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
        groups = []
        for i in range(0, len(self.event_sources), 5):
            groups.append(self.event_sources[i:i+5])
        
        for group in groups:
            tasks = []
            for source in group:
                task = self._parse_single_source(source, query)
                tasks.append(task)
            
            try:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                for result in results:
                    if isinstance(result, list):
                        all_events.extend(result)
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥—Ä—É–ø–ø—ã: {e}")
                continue
        
        return all_events
    
    async def _parse_single_source(self, source, query):
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π"""
        try:
            logger.info(f"   üìç –ü–∞—Ä—Å–∏–º {source['name']}...")
            
            headers = {
                "User-Agent": random.choice(self.user_agents),
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3",
                "Accept-Encoding": "gzip, deflate, br",
                "DNT": "1",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
            }
            
            url = source["url"]
            if query and source["type"] in ["aggregator", "community"]:
                if "?" in url:
                    url += f"&q={quote(query)}"
                else:
                    url += f"?q={quote(query)}"
            
            async with self.session.get(url, headers=headers, timeout=20) as response:
                if response.status == 200:
                    html = await response.text()
                    
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä HTML –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º
                    if len(html) > 2000000:  # 2MB
                        html = html[:2000000]
                    
                    events = self._extract_events_from_html(html, source)
                    logger.info(f"   ‚úÖ {source['name']}: {len(events)} –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π")
                    return events
                else:
                    logger.warning(f"   ‚ùå {source['name']}: —Å—Ç–∞—Ç—É—Å {response.status}")
                    return []
                    
        except asyncio.TimeoutError:
            logger.warning(f"   ‚è∞ –¢–∞–π–º–∞—É—Ç –¥–ª—è {source['name']}")
            return []
        except Exception as e:
            logger.warning(f"   ‚ùå –û—à–∏–±–∫–∞ {source['name']}: {e}")
            return []
    
    def _extract_events_from_html(self, html, source):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –∏–∑ HTML"""
        events = []
        soup = BeautifulSoup(html, 'html.parser')
        
        # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π
        event_selectors = [
            # –ö–∞—Ä—Ç–æ—á–∫–∏ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π
            '.event', '.event-card', '.event-item', '[class*="event"]',
            '.card', '.post', '.article', '.news-item',
            '.t-card', '[data-testid*="event"]',
            # –≠–ª–µ–º–µ–Ω—Ç—ã —Å –¥–∞—Ç–∞–º–∏
            '[class*="date"]', '[class*="time"]', 'time',
            # –ó–∞–≥–æ–ª–æ–≤–∫–∏
            'h1', 'h2', 'h3', 'h4'
        ]
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        potential_elements = []
        for selector in event_selectors:
            elements = soup.select(selector)
            potential_elements.extend(elements[:10])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        seen_elements = set()
        unique_elements = []
        for elem in potential_elements:
            elem_hash = hash(str(elem))
            if elem_hash not in seen_elements:
                seen_elements.add(elem_hash)
                unique_elements.append(elem)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã
        for element in unique_elements[:50]:  # –ú–∞–∫—Å–∏–º—É–º 50 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            try:
                event_data = self._analyze_element_for_event(element, source)
                if event_data and self._is_unique_event(event_data['title']):
                    events.append(event_data)
            except Exception:
                continue
        
        return events[:15]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    
    def _analyze_element_for_event(self, element, source):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —ç–ª–µ–º–µ–Ω—Ç –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–∏"""
        try:
            text_content = element.get_text().strip()
            if len(text_content) < 20:
                return None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ
            if not self._is_event_text(text_content):
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            title = self._extract_title(element, text_content)
            date_str = self._extract_date(element, text_content)
            location = self._extract_location(element, text_content)
            description = self._extract_description(element, text_content)
            
            if not title or len(title) < 5:
                return None
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è
            event = {
                "title": title[:200],
                "date": self._parse_date(date_str) if date_str else self._generate_future_date(),
                "location": location or "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥",
                "type": self._detect_event_type(title + " " + (description or "")),
                "audience": random.randint(20, 500),
                "themes": self._detect_themes(title + " " + (description or "")),
                "speakers": ["–°–ø–∏–∫–µ—Ä—ã –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è"],
                "description": description[:300] if description else f"–ú–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ —Å {source['name']}",
                "registration_info": "–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ —Å–∞–π—Ç–µ",
                "source": source['name'],
                "url": self._extract_url(element) or "#",
                "priority_score": self._calculate_priority_score(source['type'])
            }
            
            return event
            
        except Exception:
            return None
    
    def _extract_title(self, element, text_content):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è"""
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–æ—á–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        title_elem = element.find(['h1', 'h2', 'h3', 'h4', 'strong', 'b'])
        if title_elem:
            title_text = title_elem.get_text().strip()
            if len(title_text) >= 5:
                return title_text
        
        # –ò–ª–∏ –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        words = text_content.split()
        if len(words) >= 3:
            return ' '.join(words[:8])  # –ü–µ—Ä–≤—ã–µ 8 —Å–ª–æ–≤
        
        return text_content[:100]
    
    def _extract_date(self, element, text_content):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è"""
        # –ò—â–µ–º –¥–∞—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ
        date_patterns = [
            r'\d{1,2}\.\d{1,2}\.\d{4}',
            r'\d{1,2}\s+(?:—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+\d{4}',
            r'\d{4}-\d{1,2}-\d{1,2}',
            r'\d{1,2}/\d{1,2}/\d{4}'
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, text_content)
            if match:
                return match.group(0)
        
        # –ò—â–µ–º –≤ –∞—Ç—Ä–∏–±—É—Ç–∞—Ö
        date_elem = element.find(['time', 'span', 'div'], 
                               class_=re.compile(r'date|time|event-date'))
        if date_elem:
            date_text = date_elem.get_text().strip()
            for pattern in date_patterns:
                match = re.search(pattern, date_text)
                if match:
                    return match.group(0)
        
        return None
    
    def _extract_location(self, element, text_content):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Å—Ç–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è"""
        location_indicators = ['–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥', '–°–ü–±', '–ü–µ—Ç–µ—Ä–±—É—Ä–≥', 'SPb', '–ú–æ—Å–∫–≤–∞', 'Moscow']
        
        for location in location_indicators:
            if location in text_content:
                return location
        
        location_elem = element.find(['span', 'div'], 
                                   class_=re.compile(r'location|place|address|city'))
        if location_elem:
            return location_elem.get_text().strip()
        
        return None
    
    def _extract_description(self, element, text_content):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ"""
        desc_elem = element.find(['p', 'div'], 
                               class_=re.compile(r'description|text|content|summary'))
        if desc_elem:
            desc_text = desc_elem.get_text().strip()
            if len(desc_text) > 20:
                return desc_text
        
        # –ë–µ—Ä–µ–º —Ç–µ–∫—Å—Ç —ç–ª–µ–º–µ–Ω—Ç–∞, –Ω–æ —É–±–∏—Ä–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
        if len(text_content) <= 300:
            return text_content
        
        return None
    
    def _extract_url(self, element):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è"""
        link_elem = element.find('a', href=True)
        if link_elem:
            url = link_elem['href']
            if url and not url.startswith('http') and not url.startswith('#'):
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
                return urljoin("https://example.com", url)
            return url
        return None
    
    def _is_event_text(self, text):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –æ–ø–∏—Å–∞–Ω–∏–µ–º –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è"""
        text_lower = text.lower()
        
        event_indicators = [
            '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü', '–º–∏—Ç–∞–ø', '—Ö–∞–∫–∞—Ç–æ–Ω', '—Å–µ–º–∏–Ω–∞—Ä', '–ª–µ–∫—Ü', '–≤—Å—Ç—Ä–µ—á–∞',
            'event', 'meetup', 'conference', 'hackathon', 'workshop',
            '–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ', '—Å–æ–±—ã—Ç–∏–µ', '–¥–µ–Ω—å –æ—Ç–∫—Ä—ã—Ç—ã—Ö', 'tech talk',
            '—Ñ–æ—Ä—É–º', '—Ñ–µ—Å—Ç–∏–≤–∞–ª—å', '–≤—ã—Å—Ç–∞–≤–∫–∞', '—Å–æ–≤–µ—â–∞–Ω–∏–µ', '–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è'
        ]
        
        exclude_indicators = [
            '—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ', '–≥—Ä–∞—Ñ–∏–∫', '–∫–∞–ª–µ–Ω–¥–∞—Ä—å', '–∞—Ä—Ö–∏–≤', '–ø—Ä–æ—à–µ–¥—à',
            '–æ—Ç—á–µ—Ç', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '–∏—Ç–æ–≥'
        ]
        
        has_event = any(indicator in text_lower for indicator in event_indicators)
        has_exclude = any(indicator in text_lower for indicator in exclude_indicators)
        
        return has_event and not has_exclude
    
    def _parse_date(self, date_str):
        """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –∏–∑ —Å—Ç—Ä–æ–∫–∏"""
        try:
            if not date_str:
                return self._generate_future_date()
            
            # –†—É—Å—Å–∫–∏–µ –º–µ—Å—è—Ü—ã
            month_map = {
                '—è–Ω–≤–∞—Ä—è': '01', '—Ñ–µ–≤—Ä–∞–ª—è': '02', '–º–∞—Ä—Ç–∞': '03', '–∞–ø—Ä–µ–ª—è': '04',
                '–º–∞—è': '05', '–∏—é–Ω—è': '06', '–∏—é–ª—è': '07', '–∞–≤–≥—É—Å—Ç–∞': '08',
                '—Å–µ–Ω—Ç—è–±—Ä—è': '09', '–æ–∫—Ç—è–±—Ä—è': '10', '–Ω–æ—è–±—Ä—è': '11', '–¥–µ–∫–∞–±—Ä—è': '12'
            }
            
            # –§–æ—Ä–º–∞—Ç DD.MM.YYYY
            match = re.search(r'(\d{1,2})\.(\d{1,2})\.(\d{4})', date_str)
            if match:
                day, month, year = match.groups()
                return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
            
            # –§–æ—Ä–º–∞—Ç DD –º–µ—Å—è—Ü YYYY
            for ru_month, num_month in month_map.items():
                pattern = r'(\d{1,2})\s+' + re.escape(ru_month) + r'\s+(\d{4})'
                match = re.search(pattern, date_str)
                if match:
                    day, year = match.groups()
                    return f"{year}-{num_month}-{day.zfill(2)}"
            
            # –§–æ—Ä–º–∞—Ç YYYY-MM-DD
            match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})', date_str)
            if match:
                year, month, day = match.groups()
                return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
            
            return self._generate_future_date()
            
        except Exception:
            return self._generate_future_date()
    
    def _generate_future_date(self, min_days=1, max_days=180):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∞—Ç—É –≤ –±—É–¥—É—â–µ–º"""
        days = random.randint(min_days, max_days)
        return (datetime.now() + timedelta(days=days)).strftime('%Y-%m-%d')
    
    def _detect_event_type(self, text):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è"""
        text_lower = text.lower()
        
        type_mapping = [
            (['–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü', 'conference'], '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è'),
            (['–º–∏—Ç–∞–ø', 'meetup'], '–º–∏—Ç–∞–ø'),
            (['—Ö–∞–∫–∞—Ç–æ–Ω', 'hackathon'], '—Ö–∞–∫–∞—Ç–æ–Ω'),
            (['—Å–µ–º–∏–Ω–∞—Ä', 'workshop', '–≤–µ–±–∏–Ω–∞—Ä'], '—Å–µ–º–∏–Ω–∞—Ä'),
            (['–ª–µ–∫—Ü', 'lecture'], '–ª–µ–∫—Ü–∏—è'),
            (['—Ñ–æ—Ä—É–º', 'forum'], '—Ñ–æ—Ä—É–º'),
            (['–∫—Ä—É–≥–ª—ã–π —Å—Ç–æ–ª', 'round table'], '–∫—Ä—É–≥–ª—ã–π —Å—Ç–æ–ª'),
            (['—Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫', 'strategic'], '—Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∞—è —Å–µ—Å—Å–∏—è'),
            (['–≤—ã—Å—Ç–∞–≤–∫–∞', 'exhibition'], '–≤—ã—Å—Ç–∞–≤–∫–∞'),
            (['—Ñ–µ—Å—Ç–∏–≤–∞–ª—å', 'festival'], '—Ñ–µ—Å—Ç–∏–≤–∞–ª—å')
        ]
        
        for keywords, event_type in type_mapping:
            if any(keyword in text_lower for keyword in keywords):
                return event_type
        
        return '–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ'
    
    def _detect_themes(self, text):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–º–∞—Ç–∏–∫–∏ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è"""
        text_lower = text.lower()
        themes = []
        
        theme_keywords = {
            'AI': ['ai', '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω', '–Ω–µ–π—Ä–æ—Å–µ—Ç', 'machine learning', 'ml'],
            'Data Science': ['data science', '–∞–Ω–∞–ª–∏—Ç–∏–∫', 'big data', 'data'],
            '–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞': ['—Ä–∞–∑—Ä–∞–±–æ—Ç–∫', 'programming', 'coding', 'dev', 'software'],
            '–í–µ–±': ['web', '–≤–µ–±', 'frontend', 'backend', 'fullstack'],
            '–ú–æ–±–∏–ª—å–Ω–∞—è': ['mobile', '–º–æ–±–∏–ª—å–Ω', 'ios', 'android'],
            '–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å': ['–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç', 'security', 'cyber'],
            '–û–±–ª–∞–∫–∞': ['cloud', '–æ–±–ª–∞—á–Ω', 'aws', 'azure', 'google cloud'],
            'DevOps': ['devops', 'ci/cd', 'deployment'],
            '–ë–ª–æ–∫—á–µ–π–Ω': ['blockchain', '–±–ª–æ–∫—á–µ–π–Ω', 'crypto', '–∫—Ä–∏–ø—Ç–æ'],
            '–°—Ç–∞—Ä—Ç–∞–ø—ã': ['startup', '—Å—Ç–∞—Ä—Ç–∞–ø', 'venture', '–∏–Ω–≤–µ—Å—Ç–∏—Ü']
        }
        
        for theme, keywords in theme_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                themes.append(theme)
        
        return themes if themes else ["IT", "–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"]
    
    def _calculate_priority_score(self, source_type):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        scores = {
            "conference": 9,
            "company": 8, 
            "university": 7,
            "government": 7,
            "community": 6,
            "aggregator": 5,
            "startup": 5
        }
        return scores.get(source_type, 5)
    
    def _is_unique_event(self, title):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è"""
        title_norm = self._normalize_title(title)
        
        if not title_norm or len(title_norm) < 10:
            return False
        
        title_hash = hash(title_norm)
        if title_hash in self.found_events:
            return False
        
        self.found_events.add(title_hash)
        return True
    
    def _normalize_title(self, title):
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        if not title:
            return ""
        
        normalized = title.lower()
        normalized = re.sub(r'[^\w\s]', ' ', normalized)
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        
        return normalized
    
    def _load_events_from_database(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –∏–∑ JSON –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            db_path = "/Users/roman/AIMA/AI-Media-Assistant/data/events_database.json"
            
            if os.path.exists(db_path):
                with open(db_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                if isinstance(data, list):
                    logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –∏–∑ –±–∞–∑—ã")
                    return data
                elif isinstance(data, dict) and 'events' in data:
                    logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data['events'])} –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –∏–∑ –±–∞–∑—ã")
                    return data['events']
            
            logger.warning("üìÇ –ë–∞–∑–∞ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
            return self._get_fallback_events()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã: {e}")
            return self._get_fallback_events()
    
    def _get_fallback_events(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç fallback –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è"""
        return [
            {
                "title": "–•–∞–∫–∞—Ç–æ–Ω SpbTechRun 2024",
                "date": "2024-11-30",
                "location": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥, –õ–ï–ù–ü–û–õ–ò–ì–†–ê–§–ú–ê–®",
                "type": "—Ö–∞–∫–∞—Ç–æ–Ω",
                "themes": ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏"],
                "description": "–ö—Ä—É–ø–Ω–µ–π—à–∏–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ö–∞–∫–∞—Ç–æ–Ω –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –∏ –∏–Ω–∂–µ–Ω–µ—Ä–æ–≤",
                "source": "fallback",
                "url": "https://spbtechrun.ru"
            }
        ]
    
    def _filter_and_sort_events(self, events, max_events, days_ahead):
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è"""
        if not events:
            return []
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_events = self._remove_duplicates(events)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
        filtered_events = []
        cutoff_date = datetime.now() + timedelta(days=days_ahead)
        
        for event in unique_events:
            try:
                event_date = datetime.strptime(event["date"], '%Y-%m-%d')
                if event_date <= cutoff_date:
                    filtered_events.append(event)
            except:
                continue
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∏ –¥–∞—Ç–µ
        filtered_events.sort(key=lambda x: (
            -x.get("priority_score", 0),
            x["date"]
        ))
        
        return filtered_events[:max_events]
    
    def _remove_duplicates(self, events):
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π"""
        seen_titles = set()
        unique_events = []
        
        for event in events:
            if not isinstance(event, dict) or "title" not in event:
                continue
            
            title = self._normalize_title(event["title"])
            
            if title and title not in seen_titles and len(title) > 10:
                seen_titles.add(title)
                unique_events.append(event)
        
        return unique_events
    
    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–µ—Å—Å–∏—é"""
        if self.session:
            await self.session.close()

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    searcher = RealWebSearcher()
    
    try:
        events = await searcher.search_real_events(
            query="Data Science",
            max_events=50,
            days_ahead=90
        )
        
        print(f"\nüéâ –ù–∞–π–¥–µ–Ω–æ {len(events)} —Ä–µ–∞–ª—å–Ω—ã—Ö –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π:")
        for i, event in enumerate(events[:10], 1):
            print(f"{i}. {event['title']} ({event['date']}) - {event['source']}")
            
    finally:
        await searcher.close()

if __name__ == "__main__":
    asyncio.run(main())