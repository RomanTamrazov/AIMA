import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime, timedelta
import re
import random
import asyncio
import aiohttp
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
import config

class EventSources:
    def __init__(self):
        self.session = None
        self.found_events = set()
    
    async def parse_real_events(self):
        """
        –†–µ–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π —Å –∂–∏–≤—ã—Ö —Å–∞–π—Ç–æ–≤
        """
        print("üåê –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π...")
        
        if not self.session:
            self.session = aiohttp.ClientSession()
        
        try:
            all_events = []
            
            # 1. –ü–∞—Ä—Å–∏–º TimePad (—Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫)
            print("üìÖ –ü–∞—Ä—Å–∏–º TimePad...")
            timepad_events = await self._parse_timepad_real()
            all_events.extend(timepad_events)
            
            # 2. –ü–∞—Ä—Å–∏–º —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—ã
            print("üéì –ü–∞—Ä—Å–∏–º —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—ã...")
            university_events = await self._parse_universities_real()
            all_events.extend(university_events)
            
            # 3. –ü–∞—Ä—Å–∏–º IT –∫–æ–º–ø–∞–Ω–∏–∏
            print("üè¢ –ü–∞—Ä—Å–∏–º IT –∫–æ–º–ø–∞–Ω–∏–∏...")
            company_events = await self._parse_companies_real()
            all_events.extend(company_events)
            
            # 4. –ü–∞—Ä—Å–∏–º IT –ø–æ—Ä—Ç–∞–ª—ã
            print("üì∞ –ü–∞—Ä—Å–∏–º IT –ø–æ—Ä—Ç–∞–ª—ã...")
            portal_events = await self._parse_portals_real()
            all_events.extend(portal_events)
            
            # 5. –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑—É –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∫–∞–∫ fallback
            default_events = self.get_sample_events()
            all_events.extend(default_events)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            unique_events = self._remove_duplicates(all_events)
            
            print(f"‚úÖ –†–µ–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ {len(unique_events)} –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π")
            return unique_events
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            return self.get_sample_events()
    
    async def _parse_timepad_real(self):
        """–†–µ–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ TimePad"""
        events = []
        urls = [
            "https://timepad.ru/events/categories/technology/",
            "https://timepad.ru/events/categories/business/",
            "https://timepad.ru/events/categories/education/",
            "https://timepad.ru/events/list/?categories=technology&cities=spb"
        ]
        
        for url in urls:
            try:
                print(f"   üîç –ü–∞—Ä—Å–∏–º {url}")
                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
                }
                
                async with self.session.get(url, headers=headers, timeout=15) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π
                        event_cards = soup.find_all('div', class_=re.compile(r'event-card|t-card|card'))
                        
                        for card in event_cards[:15]:
                            try:
                                event_data = self._extract_timepad_event(card)
                                if event_data and self._is_unique_event_by_title(event_data['title']):
                                    events.append(event_data)
                            except Exception as e:
                                continue
                
                await asyncio.sleep(2)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ TimePad: {e}")
                continue
        
        return events
    
    def _extract_timepad_event(self, card):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –∏–∑ –∫–∞—Ä—Ç–æ—á–∫–∏ TimePad"""
        try:
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title_elem = card.find(['h3', 'h4', 'a'], class_=re.compile(r'title|name'))
            if not title_elem:
                return None
            
            title = title_elem.get_text().strip()
            if len(title) < 5:
                return None
            
            # –î–∞—Ç–∞
            date_elem = card.find(['time', 'span'], class_=re.compile(r'date|time'))
            date_text = date_elem.get_text().strip() if date_elem else ""
            
            # –ú–µ—Å—Ç–æ
            location_elem = card.find(['span', 'div'], class_=re.compile(r'location|place'))
            location = location_elem.get_text().strip() if location_elem else "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥"
            
            # –û–ø–∏—Å–∞–Ω–∏–µ
            desc_elem = card.find(['p', 'div'], class_=re.compile(r'description|text'))
            description = desc_elem.get_text().strip() if desc_elem else f"–ú–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ: {title}"
            
            # –°—Å—ã–ª–∫–∞
            link_elem = card.find('a', href=True)
            url = link_elem['href'] if link_elem else "#"
            if url and not url.startswith('http'):
                url = f"https://timepad.ru{url}"
            
            return {
                "title": title[:200],
                "date": self._parse_real_date(date_text),
                "location": location,
                "type": self._detect_event_type(title),
                "audience": random.randint(30, 500),
                "themes": self._detect_themes(title),
                "speakers": ["–°–ø–∏–∫–µ—Ä—ã –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è"],
                "description": description[:300],
                "registration_info": "–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ TimePad",
                "source": "timepad",
                "url": url,
                "priority_score": random.randint(7, 10)
            }
            
        except Exception:
            return None
    
    async def _parse_universities_real(self):
        """–†–µ–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤"""
        events = []
        universities = [
            {"name": "–ò–¢–ú–û", "url": "https://events.itmo.ru/events"},
            {"name": "–°–ü–±–ì–£", "url": "https://events.spbu.ru/"},
            {"name": "–ü–æ–ª–∏—Ç–µ—Ö", "url": "https://www.spbstu.ru/events/"},
        ]
        
        for uni in universities:
            try:
                print(f"   üéì –ü–∞—Ä—Å–∏–º {uni['name']}...")
                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                }
                
                async with self.session.get(uni['url'], headers=headers, timeout=15) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # –ò—â–µ–º –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
                        content_elements = soup.find_all(['div', 'article', 'section'], 
                                                       class_=re.compile(r'event|news|post|card'))
                        
                        for element in content_elements[:20]:
                            try:
                                text = element.get_text().strip()
                                if len(text) < 20:
                                    continue
                                
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ
                                if self._is_event_text(text):
                                    event_data = self._extract_university_event(text, uni['name'])
                                    if event_data and self._is_unique_event_by_title(event_data['title']):
                                        events.append(event_data)
                            except Exception:
                                continue
                
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {uni['name']}: {e}")
                continue
        
        return events
    
    def _extract_university_event(self, text, uni_name):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
            clean_text = re.sub(r'\s+', ' ', text).strip()
            if len(clean_text) < 15:
                return None
            
            # –ò—â–µ–º –¥–∞—Ç—É –≤ —Ç–µ–∫—Å—Ç–µ
            date_match = re.search(r'\d{1,2}\.\d{1,2}\.\d{4}', clean_text)
            
            return {
                "title": clean_text[:150] + ("..." if len(clean_text) > 150 else ""),
                "date": self._parse_real_date(date_match.group(0)) if date_match else self._generate_future_date(),
                "location": f"–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥, {uni_name}",
                "type": self._detect_event_type(clean_text),
                "audience": random.randint(50, 300),
                "themes": self._detect_themes(clean_text),
                "speakers": [f"–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–∏ {uni_name}"],
                "description": f"–ú–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ –≤ {uni_name}",
                "registration_info": f"–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ —Å–∞–π—Ç–µ {uni_name}",
                "source": f"{uni_name.lower()}_university",
                "url": "#",
                "priority_score": random.randint(6, 9)
            }
            
        except Exception:
            return None
    
    async def _parse_companies_real(self):
        """–†–µ–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ IT –∫–æ–º–ø–∞–Ω–∏–π"""
        events = []
        companies = [
            {"name": "–Ø–Ω–¥–µ–∫—Å", "url": "https://events.yandex.ru/"},
            {"name": "JetBrains", "url": "https://www.jetbrains.com/ru-ru/events/"},
            {"name": "–°–±–µ—Ä", "url": "https://sber.ru/events"},
        ]
        
        for company in companies:
            try:
                print(f"   üè¢ –ü–∞—Ä—Å–∏–º {company['name']}...")
                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                }
                
                async with self.session.get(company['url'], headers=headers, timeout=15) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # –ò—â–µ–º —Å–æ–±—ã—Ç–∏—è –Ω–∞ —Å–∞–π—Ç–∞—Ö –∫–æ–º–ø–∞–Ω–∏–π
                        event_elements = soup.find_all(['div', 'article', 'section'], 
                                                     class_=re.compile(r'event|meetup|conference'))
                        
                        for element in event_elements[:15]:
                            try:
                                text = element.get_text().strip()
                                if len(text) < 20:
                                    continue
                                
                                if self._is_event_text(text):
                                    event_data = self._extract_company_event(text, company['name'])
                                    if event_data and self._is_unique_event_by_title(event_data['title']):
                                        events.append(event_data)
                            except Exception:
                                continue
                
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {company['name']}: {e}")
                continue
        
        return events
    
    def _extract_company_event(self, text, company_name):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        try:
            clean_text = re.sub(r'\s+', ' ', text).strip()
            if len(clean_text) < 15:
                return None
            
            date_match = re.search(r'\d{1,2}\.\d{1,2}\.\d{4}', clean_text)
            
            return {
                "title": clean_text[:150] + ("..." if len(clean_text) > 150 else ""),
                "date": self._parse_real_date(date_match.group(0)) if date_match else self._generate_future_date(),
                "location": f"–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥, {company_name}",
                "type": self._detect_event_type(clean_text),
                "audience": random.randint(50, 300),
                "themes": self._detect_themes(clean_text),
                "speakers": [f"–≠–∫—Å–ø–µ—Ä—Ç—ã {company_name}"],
                "description": f"–ú–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ –æ—Ç {company_name}",
                "registration_info": f"–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ —Å–∞–π—Ç–µ {company_name}",
                "source": f"{company_name.lower()}_company",
                "url": "#",
                "priority_score": random.randint(7, 10)
            }
            
        except Exception:
            return None
    
    async def _parse_portals_real(self):
        """–†–µ–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ IT –ø–æ—Ä—Ç–∞–ª–æ–≤"""
        events = []
        portals = [
            {"name": "–•–∞–±—Ä", "url": "https://habr.com/ru/hub/events/"},
            {"name": "VC.ru", "url": "https://vc.ru/events"},
            {"name": "TAdviser", "url": "https://www.tadviser.ru/index.php/–ú–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è"},
        ]
        
        for portal in portals:
            try:
                print(f"   üì∞ –ü–∞—Ä—Å–∏–º {portal['name']}...")
                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                }
                
                async with self.session.get(portal['url'], headers=headers, timeout=15) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # –ò—â–µ–º –ø–æ—Å—Ç—ã/—Å—Ç–∞—Ç—å–∏ –æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è—Ö
                        posts = soup.find_all(['article', 'div', 'section'], 
                                            class_=re.compile(r'post|article|news|content'))
                        
                        for post in posts[:20]:
                            try:
                                text = post.get_text().strip()
                                if len(text) < 30:
                                    continue
                                
                                if self._is_event_text(text):
                                    event_data = self._extract_portal_event(text, portal['name'])
                                    if event_data and self._is_unique_event_by_title(event_data['title']):
                                        events.append(event_data)
                            except Exception:
                                continue
                
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {portal['name']}: {e}")
                continue
        
        return events
    
    def _extract_portal_event(self, text, portal_name):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ —Å –ø–æ—Ä—Ç–∞–ª–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        try:
            clean_text = re.sub(r'\s+', ' ', text).strip()
            if len(clean_text) < 20:
                return None
            
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = clean_text[:100] + "..." if len(clean_text) > 100 else clean_text
            
            date_match = re.search(r'\d{1,2}\.\d{1,2}\.\d{4}', clean_text)
            
            return {
                "title": title,
                "date": self._parse_real_date(date_match.group(0)) if date_match else self._generate_future_date(),
                "location": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥",
                "type": self._detect_event_type(clean_text),
                "audience": random.randint(30, 400),
                "themes": self._detect_themes(clean_text),
                "speakers": ["–≠–∫—Å–ø–µ—Ä—Ç—ã –∏–Ω–¥—É—Å—Ç—Ä–∏–∏"],
                "description": f"–ú–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ —Å {portal_name}",
                "registration_info": "–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ —Å–∞–π—Ç–µ",
                "source": f"{portal_name.lower()}_portal",
                "url": "#",
                "priority_score": random.randint(5, 9)
            }
            
        except Exception:
            return None
    
    def _is_event_text(self, text):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –æ–ø–∏—Å–∞–Ω–∏–µ–º –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è"""
        text_lower = text.lower()
        
        event_indicators = [
            '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü', '–º–∏—Ç–∞–ø', '—Ö–∞–∫–∞—Ç–æ–Ω', '—Å–µ–º–∏–Ω–∞—Ä', '–ª–µ–∫—Ü', '–≤—Å—Ç—Ä–µ—á–∞',
            'event', 'meetup', 'conference', 'hackathon', 'workshop',
            '–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ', '—Å–æ–±—ã—Ç–∏–µ', '–¥–µ–Ω—å –æ—Ç–∫—Ä—ã—Ç—ã—Ö', 'tech talk'
        ]
        
        return any(indicator in text_lower for indicator in event_indicators)
    
    def _parse_real_date(self, date_text):
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∞–ª—å–Ω—É—é –¥–∞—Ç—É –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        try:
            if not date_text:
                return self._generate_future_date()
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç
            formats = [
                '%d.%m.%Y', '%Y-%m-%d', '%d %B %Y', 
                '%B %d, %Y', '%d/%m/%Y', '%Y/%m/%d'
            ]
            
            for fmt in formats:
                try:
                    return datetime.strptime(date_text.strip(), fmt).strftime('%Y-%m-%d')
                except ValueError:
                    continue
            
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –∏—â–µ–º —á–∏—Å–ª–∞ –≤ —Ç–µ–∫—Å—Ç–µ
            numbers = re.findall(r'\d{1,2}\.\d{1,2}\.\d{4}', date_text)
            if numbers:
                return datetime.strptime(numbers[0], '%d.%m.%Y').strftime('%Y-%m-%d')
            
            return self._generate_future_date()
            
        except Exception:
            return self._generate_future_date()
    
    def _generate_future_date(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∞—Ç—É –≤ –±—É–¥—É—â–µ–º"""
        days = random.randint(1, 180)  # –î–æ 6 –º–µ—Å—è—Ü–µ–≤ –≤–ø–µ—Ä–µ–¥
        return (datetime.now() + timedelta(days=days)).strftime('%Y-%m-%d')
    
    def _detect_event_type(self, title):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É"""
        title_lower = title.lower()
        
        type_mapping = [
            (['–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü', 'conference'], '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è'),
            (['–º–∏—Ç–∞–ø', 'meetup'], '–º–∏—Ç–∞–ø'),
            (['—Ö–∞–∫–∞—Ç–æ–Ω', 'hackathon'], '—Ö–∞–∫–∞—Ç–æ–Ω'),
            (['—Å–µ–º–∏–Ω–∞—Ä', 'workshop', '–≤–µ–±–∏–Ω–∞—Ä'], '—Å–µ–º–∏–Ω–∞—Ä'),
            (['–ª–µ–∫—Ü', 'lecture'], '–ª–µ–∫—Ü–∏—è'),
            (['—Ñ–æ—Ä—É–º', 'forum'], '—Ñ–æ—Ä—É–º'),
            (['–∫—Ä—É–≥–ª—ã–π —Å—Ç–æ–ª', 'round table'], '–∫—Ä—É–≥–ª—ã–π —Å—Ç–æ–ª'),
            (['—Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫', 'strategic'], '—Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∞—è —Å–µ—Å—Å–∏—è')
        ]
        
        for keywords, event_type in type_mapping:
            if any(keyword in title_lower for keyword in keywords):
                return event_type
        
        return '–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ'
    
    def _detect_themes(self, title):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–º–∞—Ç–∏–∫–∏ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É"""
        title_lower = title.lower()
        themes = []
        
        theme_keywords = {
            'AI': ['ai', '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω', '–Ω–µ–π—Ä–æ—Å–µ—Ç', 'machine learning', 'ml'],
            'Data Science': ['data science', '–∞–Ω–∞–ª–∏—Ç–∏–∫', 'big data', 'data'],
            '–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞': ['—Ä–∞–∑—Ä–∞–±–æ—Ç–∫', 'programming', 'coding', 'dev', 'software'],
            '–í–µ–±': ['web', '–≤–µ–±', 'frontend', 'backend', 'fullstack'],
            '–ú–æ–±–∏–ª—å–Ω–∞—è': ['mobile', '–º–æ–±–∏–ª—å–Ω', 'ios', 'android'],
            '–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å': ['–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç', 'security', 'cyber'],
            '–û–±–ª–∞–∫–∞': ['cloud', '–æ–±–ª–∞—á–Ω', 'aws', 'azure', 'google cloud'],
            'DevOps': ['devops', 'ci/cd', 'deployment'],
            '–ë–ª–æ–∫—á–µ–π–Ω': ['blockchain', '–±–ª–æ–∫—á–µ–π–Ω', 'crypto', '–∫—Ä–∏–ø—Ç–æ'],
            '–°—Ç–∞—Ä—Ç–∞–ø—ã': ['startup', '—Å—Ç–∞—Ä—Ç–∞–ø', 'venture', '–∏–Ω–≤–µ—Å—Ç–∏—Ü']
        }
        
        for theme, keywords in theme_keywords.items():
            if any(keyword in title_lower for keyword in keywords):
                themes.append(theme)
        
        return themes if themes else ["IT", "–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"]
    
    def _remove_duplicates(self, events):
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π"""
        seen_titles = set()
        unique_events = []
        
        for event in events:
            if not isinstance(event, dict) or 'title' not in event:
                continue
                
            title = self._normalize_title(event['title'])
            
            if title and title not in seen_titles and len(title) > 10:
                seen_titles.add(title)
                unique_events.append(event)
        
        return unique_events
    
    def _normalize_title(self, title):
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        if not title:
            return ""
        
        normalized = title.lower()
        normalized = re.sub(r'[^\w\s]', ' ', normalized)
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        
        return normalized
    
    def _is_unique_event_by_title(self, title):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É"""
        title_norm = self._normalize_title(title)
        
        if not title_norm or len(title_norm) < 10:
            return False
        
        title_hash = hash(title_norm)
        if title_hash in self.found_events:
            return False
        
        self.found_events.add(title_hash)
        return True

    @staticmethod
    def get_sample_events():
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–∞–∑—É –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        try:
            os.makedirs(os.path.dirname(config.EVENTS_DB), exist_ok=True)
            
            with open(config.EVENTS_DB, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    return data
                elif isinstance(data, dict) and 'events' in data:
                    return data['events']
                else:
                    return EventSources._get_default_events()
        except FileNotFoundError:
            return EventSources._get_default_events()
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π: {e}")
            return EventSources._get_default_events()
    
    @staticmethod
    def _get_default_events():
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        return [
            {
                "title": "–•–∞–∫–∞—Ç–æ–Ω SpbTechRun 2024",
                "date": "2024-11-30",
                "location": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥, –õ–ï–ù–ü–û–õ–ò–ì–†–ê–§–ú–ê–®",
                "audience": 300,
                "type": "—Ö–∞–∫–∞—Ç–æ–Ω",
                "themes": ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏"],
                "speakers": ["–ò–¢-—ç–∫—Å–ø–µ—Ä—Ç—ã", "–ü—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–µ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã"],
                "description": "–ö—Ä—É–ø–Ω–µ–π—à–∏–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ö–∞–∫–∞—Ç–æ–Ω –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –∏ –∏–Ω–∂–µ–Ω–µ—Ä–æ–≤",
                "registration_info": "–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ —Å–∞–π—Ç–µ spbtechrun.ru",
                "source": "partner_invitation",
                "url": "https://spbtechrun.ru"
            },
            {
                "title": "Women in Data Science 2025",
                "date": "2025-03-07",
                "location": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥, –û—Ç–µ–ª—å –ö–æ—Ä–∏–Ω—Ç–∏—è",
                "audience": 250,
                "type": "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è",
                "themes": ["Data Science", "AI", "–∂–µ–Ω—â–∏–Ω—ã –≤ IT", "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"],
                "speakers": ["–õ–∏–¥–µ—Ä—ã ODS", "Data Scientist –∏–∑ —Ç–æ–ø –∫–æ–º–ø–∞–Ω–∏–π"],
                "description": "–ö—Ä—É–ø–Ω–µ–π—à–∞—è –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è –æ –∂–µ–Ω—â–∏–Ω–∞—Ö –≤ Data Science –≤ –†–æ—Å—Å–∏–∏",
                "registration_info": "–û—Ç–∫—Ä—ã—Ç–∞—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ ods.ai",
                "source": "community_event",
                "url": "https://ods.ai"
            }
        ]
    
    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–µ—Å—Å–∏—é"""
        if self.session:
            await self.session.close()