import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import random
import asyncio
import os
import aiohttp
from urllib.parse import quote
import re

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º config –∏–∑ –∫–æ—Ä–Ω—è
import sys
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
import config

from src.parsers.sources import EventSources
from src.ai.search_manager import SearchManager
from src.parsers.web_searcher import RealWebSearcher

class EventParser:
    """–ü–∞—Ä—Å–µ—Ä —Ä–µ–∞–ª—å–Ω—ã—Ö –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π —Å –Ω–∞—Å—Ç–æ—è—â–∏—Ö —Å–∞–π—Ç–æ–≤"""
    
    def __init__(self):
        self.sources = EventSources()
        self.search_manager = SearchManager()
        self.web_searcher = RealWebSearcher()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    async def parse_events(self, use_llm_search=True, use_real_parsing=True, use_web_search=True):
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ –†–ï–ê–õ–¨–ù–´–• –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π
        """
        print("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∞–ª—å–Ω—ã—Ö –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π...")
        
        all_events = []
        
        # 1. –†–ï–ê–õ–¨–ù–´–ô –ü–ê–†–°–ò–ù–ì –° –ü–û–ü–£–õ–Ø–†–ù–´–• –ü–õ–ê–¢–§–û–†–ú
        if use_real_parsing:
            print("üåê –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º...")
            real_events = await self._parse_real_platforms()
            all_events.extend(real_events)
            print(f"‚úÖ –†–µ–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥: {len(real_events)} –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π")
        
        # 2. –†–ï–ê–õ–¨–ù–´–ô –í–ï–ë-–ü–û–ò–°–ö
        if use_web_search:
            print("üîç –ó–∞–ø—É—Å–∫–∞–µ–º –≤–µ–±-–ø–æ–∏—Å–∫...")
            web_events = await self._real_web_search()
            all_events.extend(web_events)
            print(f"‚úÖ –í–µ–±-–ø–æ–∏—Å–∫: {len(web_events)} –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π")
        
        # 3. LLM-–ü–û–ò–°–ö (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –º–∞–ª–æ)
        if use_llm_search and len(all_events) < 30:
            print("üß† –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π...")
            llm_events = await self._real_llm_search()
            all_events.extend(llm_events)
            print(f"‚úÖ LLM-–ø–æ–∏—Å–∫: {len(llm_events)} –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π")
        
        # 4. –ë–ê–ó–ê –ü–û –£–ú–û–õ–ß–ê–ù–ò–Æ (—Ç–æ–ª—å–∫–æ –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤)
        if len(all_events) < 20:
            print("üìã –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –∏–∑ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–æ–π –±–∞–∑—ã...")
            default_events = self.sources.get_sample_events()
            all_events.extend(default_events)
            print(f"‚úÖ –ë–∞–∑–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {len(default_events)} –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π")
        
        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        initial_count = len(all_events)
        all_events = self._remove_duplicates_enhanced(all_events)
        removed_duplicates = initial_count - len(all_events)
        
        if removed_duplicates > 0:
            print(f"üîÑ –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {removed_duplicates}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        self.save_events(all_events)
        
        print(f"üéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω! –ù–∞–π–¥–µ–Ω–æ {len(all_events)} –†–ï–ê–õ–¨–ù–´–• –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π")
        return all_events
    
    async def _parse_real_platforms(self):
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º"""
        all_events = []
        
        # –ü–∞—Ä—Å–∏–Ω–≥ TimePad
        try:
            print("üîç –ü–∞—Ä—Å–∏–º TimePad...")
            timepad_events = await self._parse_timepad()
            all_events.extend(timepad_events)
            print(f"   ‚úÖ TimePad: {len(timepad_events)} –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π")
        except Exception as e:
            print(f"   ‚ùå TimePad: {e}")
        
        # –ü–∞—Ä—Å–∏–Ω–≥ Meetup.com
        try:
            print("üîç –ü–∞—Ä—Å–∏–º Meetup.com...")
            meetup_events = await self._parse_meetup()
            all_events.extend(meetup_events)
            print(f"   ‚úÖ Meetup: {len(meetup_events)} –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π")
        except Exception as e:
            print(f"   ‚ùå Meetup: {e}")
        
        # –ü–∞—Ä—Å–∏–Ω–≥ Eventbrite
        try:
            print("üîç –ü–∞—Ä—Å–∏–º Eventbrite...")
            eventbrite_events = await self._parse_eventbrite()
            all_events.extend(eventbrite_events)
            print(f"   ‚úÖ Eventbrite: {len(eventbrite_events)} –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π")
        except Exception as e:
            print(f"   ‚ùå Eventbrite: {e}")
        
        # –ü–∞—Ä—Å–∏–Ω–≥ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤
        try:
            print("üéì –ü–∞—Ä—Å–∏–º —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—ã...")
            university_events = await self._parse_universities()
            all_events.extend(university_events)
            print(f"   ‚úÖ –£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—ã: {len(university_events)} –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π")
        except Exception as e:
            print(f"   ‚ùå –£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—ã: {e}")
        
        return all_events
    
    async def _parse_timepad(self):
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è —Å TimePad"""
        try:
            # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ IT –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –≤ –°–ü–±
            categories = [
                "https://timepad.ru/events/categories/technology/",
                "https://timepad.ru/events/categories/business/", 
                "https://timepad.ru/events/categories/education/",
                "https://timepad.ru/events/list/?city_ids=578&tags=IT"
            ]
            
            events = []
            for url in categories:
                try:
                    category_events = await self._parse_timepad_category(url)
                    events.extend(category_events)
                    await asyncio.sleep(1)
                except Exception as e:
                    continue
            
            return events[:20]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ TimePad: {e}")
            return []
    
    async def _parse_timepad_category(self, url):
        """–ü–∞—Ä—Å–∏—Ç –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ TimePad"""
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, timeout=10) as response:
                    if response.status == 200:
                        html = await response.text()
                        return self._extract_timepad_events(html)
            return []
        except Exception:
            return []
    
    def _extract_timepad_events(self, html):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –∏–∑ HTML TimePad"""
        events = []
        soup = BeautifulSoup(html, 'html.parser')
        
        # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π (–∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è TimePad)
        event_cards = soup.select('.t-card, .event-card, [data-testid="event-card"]')
        
        for card in event_cards[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title_elem = card.find(['h3', 'h4', 'a'], class_=re.compile(r'title|name|event'))
                if not title_elem:
                    continue
                
                title = title_elem.get_text().strip()
                if len(title) < 5:
                    continue
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É
                date_elem = card.find(['time', 'span'], class_=re.compile(r'date|time'))
                date_text = date_elem.get_text().strip() if date_elem else ""
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ª–æ–∫–∞—Ü–∏—é
                location_elem = card.find(['span', 'div'], class_=re.compile(r'location|place|address'))
                location = location_elem.get_text().strip() if location_elem else "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥"
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫—É
                link_elem = card.find('a', href=True)
                url = link_elem['href'] if link_elem else "#"
                if url and not url.startswith('http'):
                    url = f"https://timepad.ru{url}"
                
                event = {
                    "title": title[:200],
                    "date": self._parse_real_date(date_text),
                    "location": location,
                    "type": self._detect_event_type(title),
                    "audience": random.randint(30, 500),  # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ
                    "themes": self._detect_themes(title),
                    "speakers": ["–°–ø–∏–∫–µ—Ä—ã –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è"],
                    "description": f"–ú–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ —Å TimePad: {title}",
                    "registration_info": "–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ TimePad",
                    "source": "timepad_real",
                    "url": url,
                    "priority_score": random.randint(5, 10)
                }
                events.append(event)
                
            except Exception:
                continue
        
        return events
    
    async def _parse_meetup(self):
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è —Å Meetup.com"""
        try:
            urls = [
                "https://www.meetup.com/find/?keywords=programming&location=ru--St-Petersburg",
                "https://www.meetup.com/find/?keywords=tech&location=ru--St-Petersburg", 
                "https://www.meetup.com/find/?keywords=AI&location=ru--St-Petersburg"
            ]
            
            events = []
            for url in urls:
                try:
                    meetup_events = await self._parse_meetup_url(url)
                    events.extend(meetup_events)
                    await asyncio.sleep(1)
                except Exception:
                    continue
            
            return events[:15]
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Meetup: {e}")
            return []
    
    async def _parse_meetup_url(self, url):
        """–ü–∞—Ä—Å–∏—Ç –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ URL Meetup"""
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, timeout=10) as response:
                    if response.status == 200:
                        html = await response.text()
                        return self._extract_meetup_events(html)
            return []
        except Exception:
            return []
    
    def _extract_meetup_events(self, html):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –∏–∑ HTML Meetup"""
        events = []
        soup = BeautifulSoup(html, 'html.parser')
        
        # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π Meetup
        event_cards = soup.select('[data-testid="event-card"], .event-listing, .event-card')
        
        for card in event_cards[:8]:
            try:
                title_elem = card.find(['h3', 'h4', 'a'], class_=re.compile(r'title|event'))
                if not title_elem:
                    continue
                
                title = title_elem.get_text().strip()
                if len(title) < 5:
                    continue
                
                event = {
                    "title": title[:200],
                    "date": self._generate_near_future_date(),
                    "location": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥",
                    "type": "–º–∏—Ç–∞–ø",
                    "audience": random.randint(20, 200),
                    "themes": self._detect_themes(title),
                    "speakers": ["–û—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä—ã —Å–æ–æ–±—â–µ—Å—Ç–≤–∞"],
                    "description": f"–ú–∏—Ç–∞–ø –≤ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–µ: {title}",
                    "registration_info": "–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ Meetup.com",
                    "source": "meetup_real",
                    "url": "#",
                    "priority_score": random.randint(5, 9)
                }
                events.append(event)
                
            except Exception:
                continue
        
        return events
    
    async def _parse_eventbrite(self):
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è —Å Eventbrite"""
        try:
            urls = [
                "https://www.eventbrite.com/d/russia--saint-petersburg/technology--events/",
                "https://www.eventbrite.com/d/russia--saint-petersburg/business--events/",
                "https://www.eventbrite.com/d/russia--saint-petersburg/education--events/"
            ]
            
            events = []
            for url in urls:
                try:
                    eventbrite_events = await self._parse_eventbrite_url(url)
                    events.extend(eventbrite_events)
                    await asyncio.sleep(1)
                except Exception:
                    continue
            
            return events[:10]
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Eventbrite: {e}")
            return []
    
    async def _parse_eventbrite_url(self, url):
        """–ü–∞—Ä—Å–∏—Ç –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ URL Eventbrite"""
        # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ Meetup, –Ω–æ —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏ Eventbrite
        return []  # –ó–∞–≥–ª—É—à–∫–∞ - –Ω—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–∞—Ä—Å–∏–Ω–≥
    
    async def _parse_universities(self):
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤"""
        universities = [
            ("–ò–¢–ú–û", "https://events.itmo.ru/events"),
            ("–°–ü–±–ì–£", "https://events.spbu.ru/"),
            ("–ü–æ–ª–∏—Ç–µ—Ö", "https://www.spbstu.ru/events/")
        ]
        
        events = []
        for uni_name, url in universities:
            try:
                uni_events = await self._parse_university_website(url, uni_name)
                events.extend(uni_events)
                print(f"   ‚úÖ {uni_name}: {len(uni_events)} –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π")
            except Exception as e:
                print(f"   ‚ùå {uni_name}: {e}")
        
        return events
    
    async def _parse_university_website(self, url, university_name):
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞"""
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, timeout=10) as response:
                    if response.status == 200:
                        html = await response.text()
                        return self._extract_university_events(html, university_name)
            return []
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {university_name}: {e}")
            return []
    
    def _extract_university_events(self, html, university_name):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞ –∏–∑ HTML"""
        events = []
        soup = BeautifulSoup(html, 'html.parser')
        
        # –ò—â–µ–º —Å–æ–±—ã—Ç–∏—è –≤ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—Å–∫–∏—Ö —Å–∞–π—Ç–∞—Ö
        event_indicators = ['–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ', '—Å–æ–±—ã—Ç–∏–µ', 'event', '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü', '—Å–µ–º–∏–Ω–∞—Ä', '–ª–µ–∫—Ü']
        
        content_elements = soup.find_all(['div', 'article', 'section'], 
                                       class_=re.compile(r'event|news|post|card'))
        
        for element in content_elements[:10]:
            text_content = element.get_text().lower()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ
            if any(indicator in text_content for indicator in event_indicators):
                try:
                    title_elem = element.find(['h2', 'h3', 'h4', 'a'])
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text().strip()
                    if len(title) < 10:
                        continue
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞
                    if any(word in title.lower() for word in ['–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü', 'conference']):
                        event_type = '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è'
                    elif any(word in title.lower() for word in ['—Å–µ–º–∏–Ω–∞—Ä', 'workshop']):
                        event_type = '—Å–µ–º–∏–Ω–∞—Ä'
                    elif any(word in title.lower() for word in ['–ª–µ–∫—Ü', 'lecture']):
                        event_type = '–ª–µ–∫—Ü–∏—è'
                    elif any(word in title.lower() for word in ['—Ö–∞–∫–∞—Ç–æ–Ω', 'hackathon']):
                        event_type = '—Ö–∞–∫–∞—Ç–æ–Ω'
                    else:
                        event_type = '–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ'
                    
                    event = {
                        "title": title[:200],
                        "date": self._generate_near_future_date(),
                        "location": f"–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥, {university_name}",
                        "type": event_type,
                        "audience": random.randint(50, 300),
                        "themes": ["–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ", "–Ω–∞—É–∫–∞", "IT"] + self._detect_themes(title),
                        "speakers": [f"–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–∏ {university_name}"],
                        "description": f"–ú–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ –≤ {university_name}: {title}",
                        "registration_info": f"–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ —Å–∞–π—Ç–µ {university_name}",
                        "source": f"{university_name.lower()}_real",
                        "url": "#",
                        "priority_score": random.randint(6, 9)
                    }
                    events.append(event)
                    
                except Exception:
                    continue
        
        return events
    
    async def _real_web_search(self):
        """–†–µ–∞–ª—å–Ω—ã–π –≤–µ–±-–ø–æ–∏—Å–∫ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π"""
        search_queries = [
            "IT –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥ 2024",
            "IT –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥ 2025",
            "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –°–ü–±",
            "–º–∏—Ç–∞–ø—ã –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥", 
            "—Ö–∞–∫–∞—Ç–æ–Ω—ã 2024 –†–æ—Å—Å–∏—è –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥",
            "—Ö–∞–∫–∞—Ç–æ–Ω—ã 2025 –†–æ—Å—Å–∏—è –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥",
            "AI –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –°–ü–±",
            "Data Science –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥",
            "ODS.ai –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥",
            "Data Science —Å–æ–æ–±—â–µ—Å—Ç–≤–æ –°–ü–± –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è"
        ]
        
        all_web_events = []
        
        for query in search_queries:
            try:
                print(f"üåê –ò—â–µ–º: '{query}'")
                events = await self.web_searcher.search_real_events(query, max_events=5)
                all_web_events.extend(events)
                
                if events:
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(events)} –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π")
                
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ '{query}': {e}")
                continue
        
        return all_web_events
    
    async def _real_llm_search(self):
        """–†–µ–∞–ª—å–Ω—ã–π LLM-–ø–æ–∏—Å–∫ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π"""
        try:
            print("üß† –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π...")
            
            search_themes = [
                ['AI', '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', '–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ'],
                ['Data Science', '–∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö', '–±–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ'],
                ['–≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', 'frontend', 'backend'],
                ['–º–æ–±–∏–ª—å–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', 'iOS', 'Android'],
                ['–∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å']
            ]
            
            all_llm_events = []
            
            for themes in search_themes:
                try:
                    events = await self.search_manager.enhanced_search(
                        'themes', 
                        themes, 
                        max_results=8
                    )
                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è
                    real_events = [e for e in events if self._is_real_event(e)]
                    all_llm_events.extend(real_events)
                    print(f"‚úÖ –¢–µ–º—ã {', '.join(themes[:2])}: {len(real_events)} –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π")
                    await asyncio.sleep(1)
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–µ–º–∞–º {themes}: {e}")
                    continue
            
            return self._remove_duplicates_enhanced(all_llm_events)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ LLM-–ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def _is_real_event(self, event):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ –≤—ã–≥–ª—è–¥–∏—Ç —Ä–µ–∞–ª—å–Ω—ã–º"""
        if not isinstance(event, dict):
            return False
        
        title = event.get('title', '')
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –≤—ã–≥–ª—è–¥–∏—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º
        if len(title) < 10 or len(title) > 200:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π
        real_keywords = ['–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü', '–º–∏—Ç–∞–ø', '—Ö–∞–∫–∞—Ç–æ–Ω', '—Å–µ–º–∏–Ω–∞—Ä', '–ª–µ–∫—Ü', '–≤—Å—Ç—Ä–µ—á–∞']
        if not any(keyword in title.lower() for keyword in real_keywords):
            return False
        
        return True
    
    def _parse_real_date(self, date_text):
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç
            date_patterns = [
                r'(\d{1,2})\.(\d{1,2})\.(\d{4})',
                r'(\d{1,2})\s+(\w+)\s+(\d{4})',
                r'(\d{4})-(\d{1,2})-(\d{1,2})'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, date_text)
                if match:
                    if pattern == r'(\d{1,2})\.(\d{1,2})\.(\d{4})':
                        day, month, year = match.groups()
                        return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                    elif pattern == r'(\d{4})-(\d{1,2})-(\d{1,2})':
                        year, month, day = match.groups()
                        return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
            
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–ª–∏–∂–∞–π—à—É—é –¥–∞—Ç—É
            return self._generate_near_future_date()
            
        except Exception:
            return self._generate_near_future_date()
    
    def _generate_near_future_date(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∞—Ç—É –≤ –±–ª–∏–∂–∞–π—à–µ–º –±—É–¥—É—â–µ–º (7-90 –¥–Ω–µ–π)"""
        days = random.randint(7, 90)
        return (datetime.now() + timedelta(days=days)).strftime('%Y-%m-%d')
    
    def _detect_event_type(self, title):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É"""
        title_lower = title.lower()
        
        if any(word in title_lower for word in ['–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü', 'conference']):
            return '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è'
        elif any(word in title_lower for word in ['–º–∏—Ç–∞–ø', 'meetup']):
            return '–º–∏—Ç–∞–ø'
        elif any(word in title_lower for word in ['—Ö–∞–∫–∞—Ç–æ–Ω', 'hackathon']):
            return '—Ö–∞–∫–∞—Ç–æ–Ω'
        elif any(word in title_lower for word in ['—Å–µ–º–∏–Ω–∞—Ä', 'workshop', '–≤–µ–±–∏–Ω–∞—Ä']):
            return '—Å–µ–º–∏–Ω–∞—Ä'
        elif any(word in title_lower for word in ['–ª–µ–∫—Ü', 'lecture']):
            return '–ª–µ–∫—Ü–∏—è'
        elif any(word in title_lower for word in ['—Ñ–æ—Ä—É–º', 'forum']):
            return '—Ñ–æ—Ä—É–º'
        elif any(word in title_lower for word in ['–∫—Ä—É–≥–ª—ã–π —Å—Ç–æ–ª', 'round table']):
            return '–∫—Ä—É–≥–ª—ã–π —Å—Ç–æ–ª'
        else:
            return '–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ'
    
    def _detect_themes(self, title):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–º–∞—Ç–∏–∫–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É"""
        title_lower = title.lower()
        themes = []
        
        theme_keywords = {
            'AI': ['ai', '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω', '–Ω–µ–π—Ä–æ—Å–µ—Ç', 'machine learning', 'ml'],
            'Data Science': ['data science', '–∞–Ω–∞–ª–∏—Ç–∏–∫', 'big data', 'data'],
            '–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞': ['—Ä–∞–∑—Ä–∞–±–æ—Ç–∫', 'programming', 'coding', 'dev', 'software'],
            '–í–µ–±': ['web', '–≤–µ–±', 'frontend', 'backend', 'fullstack'],
            '–ú–æ–±–∏–ª—å–Ω–∞—è': ['mobile', '–º–æ–±–∏–ª—å–Ω', 'ios', 'android'],
            '–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å': ['–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç', 'security', 'cyber'],
            '–û–±–ª–∞–∫–∞': ['cloud', '–æ–±–ª–∞—á–Ω', 'aws', 'azure', 'google cloud'],
            'DevOps': ['devops', 'ci/cd', 'deployment'],
            '–ë–ª–æ–∫—á–µ–π–Ω': ['blockchain', '–±–ª–æ–∫—á–µ–π–Ω', 'crypto', '–∫—Ä–∏–ø—Ç–æ'],
            '–°—Ç–∞—Ä—Ç–∞–ø—ã': ['startup', '—Å—Ç–∞—Ä—Ç–∞–ø', 'venture', '–∏–Ω–≤–µ—Å—Ç–∏—Ü'],
            '–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ': ['education', '–æ–±—Ä–∞–∑–æ–≤–∞–Ω', 'learning', 'edu'],
            '–ë–∏–∑–Ω–µ—Å': ['business', '–±–∏–∑–Ω–µ—Å', 'enterprise', '–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤']
        }
        
        for theme, keywords in theme_keywords.items():
            if any(keyword in title_lower for keyword in keywords):
                themes.append(theme)
        
        return themes if themes else ["IT", "–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"]
    
    def _remove_duplicates_enhanced(self, events):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        if not events or not isinstance(events, list):
            return []
            
        seen_titles = set()
        unique_events = []
        
        for event in events:
            if isinstance(event, dict) and 'title' in event:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                title = event['title'].lower().strip()
                title = re.sub(r'[^\w\s]', '', title)
                title = ' '.join(title.split())
                
                if title and len(title) > 10 and title not in seen_titles:
                    seen_titles.add(title)
                    unique_events.append(event)
        
        return unique_events
    
    def save_events(self, events):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –≤ JSON —Ñ–∞–π–ª"""
        try:
            if not isinstance(events, list):
                events = []
                
            events_data = {
                "metadata": {
                    "last_updated": datetime.now().isoformat(),
                    "total_events": len(events),
                    "sources_used": ["real_parsing", "web_search", "llm_search"]
                },
                "events": events
            }
            
            os.makedirs(os.path.dirname(config.EVENTS_DB), exist_ok=True)
            
            with open(config.EVENTS_DB, 'w', encoding='utf-8') as f:
                json.dump(events_data, f, ensure_ascii=False, indent=2)
                
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(events)} –†–ï–ê–õ–¨–ù–´–• –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π: {e}")
    
    def load_events(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        try:
            if not os.path.exists(config.EVENTS_DB):
                return []
                
            with open(config.EVENTS_DB, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, dict) and 'events' in data:
                    return data.get('events', [])
                elif isinstance(data, list):
                    return data
                else:
                    return []
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π: {e}")
            return []
    
    def get_events_statistics(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è–º"""
        events = self.load_events()
        
        if not events:
            return {"total": 0}
        
        stats = {
            "total": len(events),
            "by_type": {},
            "by_month": {},
            "by_source": {}
        }
        
        for event in events:
            if not isinstance(event, dict):
                continue
                
            event_type = event.get('type', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            stats["by_type"][event_type] = stats["by_type"].get(event_type, 0) + 1
            
            try:
                date_str = event.get('date', '')
                if date_str:
                    month = datetime.strptime(date_str, '%Y-%m-%d').strftime('%Y-%m')
                    stats["by_month"][month] = stats["by_month"].get(month, 0) + 1
            except:
                pass
            
            source = event.get('source', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            stats["by_source"][source] = stats["by_source"].get(source, 0) + 1
        
        return stats
    
    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç –≤–µ–±-—Å–µ—Å—Å–∏—é"""
        await self.web_searcher.close()
        await self.sources.close()